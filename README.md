# EnsNet CNN - Implementation 
 Ensemble Learning in CNN Augmented with Fully Connected Subnetworks∗

# EnsNet TensorFlow Implementation

This repository contains a third party TensorFlow implementation  of EnsNet, a novel convolutional neural network (CNN) architecture augmented with Fully Connected Subnetworks (FCSNs), as described in [[Ensemble Learning in CNN Augmented with Fully Connected
Subnetworks](https://www.jstage.jst.go.jp/article/transinf/E106.D/7/E106.D_2022EDL8098/_pdf)]. 

EnsNet is designed to enhance image recognition performance by leveraging a base CNN combined with multiple FCSNs, improving accuracy through ensemble learning techniques to then use a majority voting count.
##

![EnsNet Architecture](/Images/architecture.png)

## Table of Contents
- [Introduction](#introduction)
- [Installation](#installation)
- [Usage](#usage)
- [Features](#features)
- [Dependencies](#dependencies)
- [Configuration](#configuration)
- [Documentation](#documentation)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Contributors](#contributors)
- [License](#license)

## Introduction
EnsNet introduces an innovative approach to deep learning models for image recognition tasks. By dividing the feature maps generated by the last convolutional layer of a base CNN among multiple Fully Connected Subnetworks (FCSNs), EnsNet leverages ensemble learning within a single model architecture. This method significantly enhances the model's predictive accuracy on challenging datasets like MNIST, Fashion-MNIST, and CIFAR-10.


## Installation
Make sure you have the necessary dependencies for running this implementation, please follow the steps below:
```bash

pip install tensorflow tensorflow-addons
pip install keras-preprocessing
pip install cloud-tpu-client
pip install dropconnect-tensorflow



```
## Method Overview

![Overview](/Images/details.png)
##

The size of each convolutional layer is denoted as “Conv⟨ receptive field size⟩-⟨number of
channels⟩”, and the size of each fully connected layer is denoted as “FC-⟨number of nodes⟩”. The
ReLU activation function is used in both models but is not shown in this table for simplicity


## Configuration
The parameters of the Adam optimizer were set as follows:
α = 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10−8, and a weight decay was set to 0.

Epochs: 1300 and Batch Size: 32


```bash
adamw_optimizer = tfa.optimizers.AdamW(
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-8,
        weight_decay=0.0 
)
```


Support for TPU-based training to enhance computational performance.

Example notebooks for training and evaluating EnsNet on MNIST, Fashion-MNIST, and CIFAR-10 datasets.D

## References

Daiki Hirata
Industrial Technology Center of Okayama Prefecture
daiki_hirata@pref.okayama.lg.jp

Norikazu Takahashi
Graduate School Natural Science and Technology, Okayama University
takahashi@cs.okayama-u.ac.jp

https://arxiv.org/abs/2003.08562

https://arxiv.org/pdf/1708.04552.pdf


